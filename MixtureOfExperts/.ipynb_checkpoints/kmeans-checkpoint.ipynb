{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n",
      "2\n",
      "(63000, 784)\n",
      "(63000, 784)\n",
      "(63000, 784)\n",
      "(63000, 784)\n",
      "(63000, 784)\n",
      "(63000, 784)\n",
      "(63000, 784)\n",
      "(63000, 784)\n",
      "(63000, 784)\n",
      "(63000, 784)\n",
      "3\n",
      "(63000, 784)\n",
      "(63000, 784)\n",
      "(63000, 784)\n",
      "(63000, 784)\n",
      "(63000, 784)\n",
      "(63000, 784)\n",
      "(63000, 784)\n",
      "(63000, 784)\n",
      "(63000, 784)\n",
      "(63000, 784)\n",
      "[[52.328125 43.921875 47.4375   45.234375 43.96875  42.84375  47.5\n",
      "  49.203125 43.0625   46.3125  ]\n",
      " [69.515625 90.890625 71.921875 73.265625 84.171875 72.       78.234375\n",
      "  74.546875 63.28125  79.96875 ]]\n"
     ]
    }
   ],
   "source": [
    "#trying to implement ensemble method\n",
    "#https://datascience.stackexchange.com/questions/27169/taking-average-of-multiple-neural-networks\n",
    "#mixture of experts 'with kmeans'\n",
    "#https://en.wikipedia.org/wiki/Mixture_of_experts\n",
    "#combining models together university of Tartu\n",
    "#https://courses.cs.ut.ee/MTAT.03.277/2014_fall/uploads/Main/deep-learning-lecture-9-combining-multiple-neural-networks-to-improve-generalization-andres-viikmaa.pdf\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "import time\n",
    "\n",
    "(trX, trY), (teX, teY) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "trX = trX.reshape(60000, 784)\n",
    "teX = teX.reshape(10000, 784)\n",
    "trX = np.vstack((trX, teX))\n",
    "\n",
    "\n",
    "print(trX.shape)\n",
    "\n",
    "folds = 10\n",
    "size_of_fold = int(len(trX)/folds)\n",
    "\n",
    "average_time_to_train = []\n",
    "times_to_train = []\n",
    "\n",
    "begin = 2\n",
    "end = 11\n",
    "\n",
    "for i in range(begin, end):\n",
    "    average_time_to_train.append(0)\n",
    "    times_to_train.append([])\n",
    "\n",
    "for i in range(begin, end):\n",
    "    print(i)\n",
    "    for fold in range(0,folds):\n",
    "        temp = np.vstack((trX[0:fold*size_of_fold], trX[(fold + 1)*size_of_fold:len(trX)]))\n",
    "        print(temp.shape)\n",
    "        time_start = time.process_time()\n",
    "        kmeans = KMeans(n_clusters=i, random_state=0).fit(temp)\n",
    "        time_stop = time.process_time()\n",
    "        #average_time_to_train[fold-begin] = average_time_to_train[fold-begin] + (time_stop - time_start)/folds\n",
    "        times_to_train[i-begin].append(time_stop - time_start)\n",
    "        np.save('kmeansclusters/' + str(i) + 'fold' + str(fold) + '.npy', kmeans.cluster_centers_)\n",
    "\n",
    "#average_time_to_train = np.asarray(average_time_to_train)\n",
    "times_to_train = np.asarray(times_to_train)\n",
    "\n",
    "#print(average_time_to_train)\n",
    "print(times_to_train)\n",
    "\n",
    "times_to_train = np.asarray(times_to_train)\n",
    "\n",
    "np.save('average_time_to_cluster_data.npy', average_time_to_train)\n",
    "np.save('times_to_train.npy', times_to_train)\n",
    "\n",
    "#https://www.datascience.com/blog/learn-data-science-intro-to-data-visualization-in-matplotlib\n",
    "#ctrl f for confidence intervals\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
